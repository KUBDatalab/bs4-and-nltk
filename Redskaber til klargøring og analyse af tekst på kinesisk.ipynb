{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kurset gennemgår grundlæggende Python-kode, der kan få dig i gang med at bruge programmering som redskab til tekstbehandling, kvantitative analyser og tekst- og datamining.\n",
    "\n",
    "Mere teknisk fortalt gennemgår vi begreber som variabler, værdier, datatyperne tekststrenge, lister og loops. \n",
    "\n",
    "Vi gennemgår et eksempel på, hvordan man kan hente tekstdata, klargøre data og bruge biblioteket jieba. Jieba anvendes til at opdele tekst i ord og undersøtter traditionelt kinesisk. \n",
    "\n",
    "_Kilde: https://github.com/fxsjy/jieba_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import af biblioteker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Til navigation på computerne\n",
    "import os\n",
    "\n",
    "# Webscrape biblioteker\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Til klargøring og analyse\n",
    "import jieba\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi laver en variabel, som vi bruger til at gemme url'en til den sider, som vi vil webscrabe. \n",
    "\n",
    "Vi skal scrape denne wikipidiaside: 反对逃犯条例修订草案运动"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gem url'en i en variabel\n",
    "url_zh = 'https://zh.wikipedia.org/zh-cn/%E5%8F%8D%E5%B0%8D%E9%80%83%E7%8A%AF%E6%A2%9D%E4%BE%8B%E4%BF%AE%E8%A8%82%E8%8D%89%E6%A1%88%E9%81%8B%E5%8B%95'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi indsætter et af variablenavnene i request.get('url') nedenfor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hent data\n",
    "page = requests.get(url_zh)\n",
    "\n",
    "# scrape websiden\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find alle 'headline3 og paragraph-tags'\n",
    "tags = soup.find_all(['h1', 'h3', 'p'])\n",
    "\n",
    "# læs teksten ud af p_tags og 'join' den returnerede liste i variablen 'text'\n",
    "text = ' '.join([p.get_text() for p in tags]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klargøring af tekst "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rensning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teksten bestå af latinske bogstaver og kinesiske tegn.\n",
    "\n",
    "Ønsker man at sortere de latiske bogstaver fra kan man bruge koden nedenfor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Kilder:_\n",
    "    \n",
    "_https://stackoverflow.com/questions/2718196/find-all-chinese-text-in-a-string-using-python-and-regex_\n",
    "\n",
    "_https://unicode-table.com/en/blocks/cjk-unified-ideographs/_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_list = re.findall(r'[\\u4e00-\\u9fff]+', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (chinese_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listen kan man efterfølgende samle til en tekststreng igen med .join()\n",
    "\n",
    "_Kilde: \"https://www.w3schools.com/python/ref_string_join.asp\"_   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_text = ' '.join(chinese_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (chinese_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tekstdeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I jieba.lcut metoden indsætter vi en tekststreng eller, som i dette tilfælde, en variabel som indeholder en tekststreng, og vi styrer cut-tilstanden. L'et i .lcut() henviser til at metoden returerer en liste. 'Cut_all=True' skulle give flest mulige orddelinger af teksten, være hurtig, men mindre præcis. 'Cut_all=False' skulle være mere nøjagtige end den første, og dermed mere velegnet til tekstanalyse. _Kilde: https://github.com/fxsjy/jieba_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_list1 = jieba.lcut(chinese_text, cut_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_list1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi får returneret mange felter, der består af 'white_space'.\n",
    "For at så disse linjer fjernet fra vores data bruger vi 'if' til at sætte en betingelse ind i koden. Vi skriver, hvis vores linjer består af tegn, som ikke er lig med 'white space', så er vi interesseret i at gemme det i vaiablen seg_list2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_list2 = [i for i in seg_list1 if i != ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (seg_list2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som det fremgår ovenfor bliver lister lavet vha. firkantede parenteser ( [ ] ).\n",
    "\n",
    "Man kan tilgå elementerne i listen ved at referere til indekstallet. Igen kan vi bruge både positive og nagative tal. Husk at i Python er første indextal 0 og ikke 1, hvilket betyder, at vi tilgår det første og det sidste element i listen på denne måde:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (seg_list2[0])\n",
    "print (seg_list2[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging (POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jiebas part of speech tagger returnerer ordene og tags i to forskellige elementer. For at bruge pos-taggeren skal man importere 'import jieba.posseg as pseg'.\n",
    "\n",
    "Ifølge dokumentationen bruger man pseg efterfuldt af .cut( 'tekst_streng' ).\n",
    "_Kilde: \"4. Part of Speech Tagging https://github.com/fxsjy/jieba\"_\n",
    "\n",
    "Vi får returneret ord og tags. De ligger i .word og .flag. I dokumentationen viser programmøren, hvordan man printer ord og tags, men jeg vil gerne have alle ord og tags gemt som par i en liste. Derfor bruger jeg en tuple, som er en python datatype og tilføjer hvert ord og tag par til en liste, som jeg kalder for 'pos'.\n",
    "_Kilde: Python Tuples https://www.w3schools.com/python/python_tuples.asp_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.posseg as pseg\n",
    "words = pseg.cut(chinese_text)\n",
    "pos_tags = []\n",
    "for w in words:\n",
    "    if w.word > ' ':\n",
    "        word_tag = tuple((w.word, w.flag))\n",
    "        pos_tags.append(word_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeg skriver et for loop, der indeholder en betingelse ('if'). Med loopet gennemgår jeg listen med tuples. Hvis første element i 'tuplen' ([1]) er lig med 'v' tilføjer jeg første element ([0]) til listen 'words'.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for item in pos_tags:\n",
    "    if item[1] == 'v':\n",
    "        words.append(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Før vi ser på fordelingen af ord med 'v'-tags, er vi nødt til at gøre noget ved at python som default ikke kan printet kinesiske tegn. Derfor importerer vi 'matplotlib.pyplot as plt' og ændrer font.family til \"Microsoft YaHei\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"font.family\"] = \"Microsoft YaHei\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Herefter kan vi importere nltk og anvende nltk.FreqDist()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.FreqDist(pos_tags).plot(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opgave: prøv at udskifte 'v' med andre tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopord\n",
    "\n",
    "Stopord er småord, som ofte ikke er betydningsbærende ord.\n",
    "\n",
    "Vi har defor brug for at indlæse en stopordsliste. Den ligger i mappen stopwords, og den er fundet på jiebas github side: https://github.com/fxsjy/jieba/tree/master/extra_dict.\n",
    "\n",
    "Vi bruger os biblioteket til at navigere over i stopordsmappen. Hver kan det blive lidt indviklet, fordi når det kommer til at navigere mellem mapper, så er der en lille forskel på om, man sidder med en mac eller en pc.\n",
    "\n",
    "Jeg sidder med en pc og skal bruge '\\\\' som separator i min sti til min mappe, men hvis du har en  mac, så skal du bruge '/' som din separator i i stedet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find din separator\n",
    "os.sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find din nuværende mappe\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find ud af, hvad der ligger i mappen\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flyt dig hen i mappen med stopord\n",
    "os.chdir('.\\\\stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find din nuværende mappe igen\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find ud af, hvad der ligger i mappen - igen\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi er klar til at indlæse vores stopord."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_ch = open('stopwords_ch.txt', 'r', encoding='utf-8-sig').read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_ch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu kan alle teksterne bliver filtreret for stopord."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens = []\n",
    "for word in seg_list2:\n",
    "    if word not in sw_ch:\n",
    "        filtered_tokens.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Med en ny ordliste, der ikke længere indeholder stopord, kan vi få overblik over, hvilke betydningsbærende ord, der flyder mest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_filtered = nltk.FreqDist(filtered_tokens).plot(20, title='Hyppigste ord (uden stopord)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_tokens = []\n",
    "\n",
    "for word in filtered_tokens:\n",
    "    if len(word) > 4:\n",
    "        long_tokens.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_filtered = nltk.FreqDist(long_tokens).plot(20, title='Længste ord')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK metoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeg har benyttet nltk mange gange, men aldrig med kinesisk tekst. Vi eksperimenterer og opretter et nltk-tekstobjekt, som burde give os mulighed for at anvende forskellige nltk-metoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text = nltk.Text(seg_list2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collocation_list() returnerer en liste over de mest almindelige ordpar i teksten. Bemærk, at i nogle versioner af Python virker collocation_list() ikke. Hvis dette er tilfældet, prøv _collocations()_ i stedet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text.collocation_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concordance()-metoden returnerer konteksten af et specifikt udtryk. Længden af output kan ændres med parametrene i width og lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text.concordance('和', lines=30, width=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For at identificere ord, der optræder i en lignende kontekst, kan vi bruge metoden similar().\n",
    "\n",
    "Jeg har en forestilling om at metoden giver bedre resultater jo længere teksten er."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text.similar('和')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate() metoden kan du bruge til at genere mere eller mindre sammenhængende tekst med udgangspunkt i en eksisterende tekst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_gen = nltk_text.generate(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
