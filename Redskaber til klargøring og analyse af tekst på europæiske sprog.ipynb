{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kurset gennemgår grundlæggende Python-kode, der kan få dig i gang med at bruge programmering som redskab til tekstbehandling, kvantitative analyser og tekst- og datamining.\n",
    "\n",
    "Mere teknisk fortalt gennemgår vi begreber som variabler, værdier, datatyperne tekststrenge, lister og loops. \n",
    "\n",
    "Vi gennemgår et eksempel på, hvordan man kan hente tekstdata, klargøre data og bruge NLTK biblioteket til sin analyse. NLTK er meget udbredt inden for digital tekstanalyse, og bibliotekernes metoder er f.eks. beskrevet her. https://www.nltk.org/book/ \n",
    "\n",
    "Samtidigt lærer du om python-programmet Jupyter Notebook, så opnår du også et kendskab til det."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import af biblioteker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I python importerer man ofte biblioteker for at supplere med nogle flere metoder. Det er smart, fordi det er huritgere at programmere, når man ikke skal programmere alt fra bunden. Der følger mange biblioteker med, når man installerer Anaconda. Bibliotekerne bliver også nogle gange kaldt for pakker eller moduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Til navigation på computerne\n",
    "import os\n",
    "\n",
    "# Webscrape biblioteker\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Til klargøring og analyse\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi laver nogle variabler, som vi bruger til at gemme de forskellige url'er til de sider, som vi vil webscrabe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gem url'en i en variabel\n",
    "url_en = 'https://en.wikipedia.org/wiki/2019%E2%80%932020_Hong_Kong_protests'\n",
    "url_de = 'https://de.wikipedia.org/wiki/Proteste_in_Hongkong_2019/2020'\n",
    "url_da = 'https://da.wikipedia.org/wiki/Demonstrationerne_i_Hongkong_2019-2020'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi indsætter et af variablenavnene i request.get('url') nedenfor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hent data\n",
    "page = requests.get(url_da)\n",
    "\n",
    "# scrape websiden\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find  headline1 og 3, samt paragraph-tags'\n",
    "tags = soup.find_all(['h3', 'h1', 'p'])\n",
    "\n",
    "# læs teksten ud af p_tags og 'join' den returnerede liste i variablen 'text'\n",
    "text = ' '.join([p.get_text() for p in tags]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variablen 'text' er en 'string.' \n",
    "\n",
    "En string laver man ved at bruge citationstegn. Man kan enten bruge enkelt sitationstegn eller dobbelt citationstegn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vælg et udsnit af teksten\n",
    "\n",
    "Hvert enkelt tegn i en string (tekststeng) har et indekstal. Bemærk at i Python er første indextal 0 og ikke 1. Vi kan hente en udsnit af tekststrengen ved at skrive string[første indextal : andet indextal].\n",
    "\n",
    "F.eks. retunerer string[0:50] de første 50 tegn.\n",
    "\n",
    "Vi kan også bruge negative tal.\n",
    "\n",
    "F.eks. retunerer string[-50:] de sidste 50 tegn.\n",
    "\n",
    "Nedenfor bruger vi variabler vis værdi er lig med et tal i stedet for et tal. Det går nemlig også. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text[500:3500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text[500:3500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klargøring af tekst \n",
    "\n",
    "## Rensning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kodenstykket neden for bruger vi til at rense teksten. Koden gør brug strengmetoden .replace(), af biblioteket RegEx og strengmetoden .lower() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text_clean1 = text.replace('_','').replace(',','')\n",
    "text_clean2 = re.sub(r'\\[\\d+?\\]', ' ', text_clean1)\n",
    "text_clean3 = ' '.join(re.findall(r'\\b(\\S+)\\b', text_clean2) )\n",
    "text_lower = text_clean3.lower() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisering og lister"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lister bliver benyttet til at gemme flere værdier i en variabel. Neden for bruger vi nltk.word_tokenize(), der retunerer en liste.For at bruge nltk.word_tokenize() skal vi først importere biblioteket nltk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenisering af tekst betyder en opslitning af teksten til en liste af ord.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = nltk.word_tokenize(text_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som det fremgår ovenfor bliver lister lavet vha. firkantede parenteser ( [ ] ).\n",
    "\n",
    "Man kan tilgå elementerne i listen ved at referere til indekstallet. Igen kan vi bruge både positive og nagative tal. Husk at i python er første indextal 0 og ikke 1, hvilket betyder, at vi tilgår det første og det sidste element i listen på denne måde:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (tokenized_text[0])\n",
    "print (tokenized_text[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forbered teksten til nltk metoderne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For at kunne arbejde med vores tekstdata skal vi bearbejde vores tekststreng lidt.\n",
    "\n",
    "Først konverterer vi teksten til en liste over tokens med nltk word_tokenize()-funktionen (det har vi gjort ovenfor). Vi opretter også et nltk-tekstobjekt, som giver os mulighed for at anvende forskellige nltk-metoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text = nltk.Text(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nltk-tekstobjektet bliver produceret fra en list af tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts of Speech Tagging (POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS tagging er grammatisk tagging. Der bliver tildelt et grammatisk tag til hvert ord. For at bruge nltk's pos tagger forudsætter det at vi har et nltk tekstobjekt, som vi har produceret ovenfor.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part-of-speech tagging af ord i tokeniserede afsnit\n",
    "tagged_text = nltk.pos_tag(nltk_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loops bruger man til at gentage den samme handling. I denne sammenhæng vil vi prøve at bruge et loop til gennemse listen tagged_text for et udvalgt POS tag.\n",
    "\n",
    "Vi begynder med en tom liste. Lister laver vi med firkantede parenteser.\n",
    "\n",
    "Til loopets syntaks kan vi oversætte til: ' For hver element i variablen tagged_text, hvis elementets anden værdi er lige med 'NN', så tag den tomme liste og tilføj elementets første værdi dertil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags = []\n",
    "for item in tagged_text:\n",
    "    if item[1] == 'NN':\n",
    "        pos_tags.append(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lad os se på fordelingen af tags\n",
    "nltk.FreqDist(pos_tags).plot(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Opgave: prøv at udskifte 'NN' med andre tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopord\n",
    "\n",
    "Stopord er småord, som ofte ikke er betydningsbærende ord.\n",
    "\n",
    "Vi har defor brug for at indlæse stopordslister. De ligger i mappen stopwords.\n",
    "\n",
    "Vi bruger os biblioteket til at navigere over i stopordsmappen. Hver kan det blive lidt indviklet, fordi når det kommer til at navigere mellem mapper, så er der en lille forskel på om, man sidder med en mac eller en pc.\n",
    "\n",
    "Jeg sidder med en pc og skal bruge '\\\\' som separator i min sti til min mappe, men hvis du har en  mac, så skal du bruge '/' som din separator i i stedet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find din separator\n",
    "os.sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find din nuværende mappe\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find ud af, hvad der ligger i mappen\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flyt dig hen i mappen med stopord\n",
    "os.chdir('.\\\\stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find din nuværende mappe igen\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find ud af, hvad der ligger i mappen - igen\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi er klar til at indlæse vores stopord i tre forskellige variabler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_de, sw_dk, sw_gb = [open(i, 'r', encoding='utf-8-sig').read().split() for i in os.listdir()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu kan alle teksterne bliver filtreret for stopord."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens = []\n",
    "for word in nltk_text:\n",
    "    if word not in sw_dk and word.isalpha():\n",
    "        filtered_tokens.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Med en ny ordliste, der ikke længere indeholder stopord, kan vi få overblik over, hvilke betydningsbærende ord, der flyder mest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_filtered = nltk.FreqDist(filtered_tokens).plot(20, title='Hyppigste ord (uden stopord)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_tokens = []\n",
    "\n",
    "for word in filtered_tokens:\n",
    "    if len(word) > 10:\n",
    "        long_tokens.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_filtered = nltk.FreqDist(long_tokens).plot(20, title='Længste ord')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK metoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collocation_list() returnerer en liste over de mest almindelige ordpar i teksten. Bemærk, at i nogle versioner af Python virker collocation_list() ikke. Hvis dette er tilfældet, prøv _collocations()_ i stedet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text.collocation_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concordance()-metoden returnerer konteksten af et specifikt udtryk. Længden af output kan ændres med parametrene i width og lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text.concordance('lovforslag', lines=30, width=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For at identificere ord, der optræder i en lignende kontekst, kan vi bruge metoden similar().\n",
    "\n",
    "Jeg har en forestilling om at metoden giver bedre resultater jo længere teksten er."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_text.similar('demonstranter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate() metoden kan du bruge til at genere mere eller mindre sammenhængende tekst med udgangspunkt i en eksisterende tekst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_gen = nltk_text.generate(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
